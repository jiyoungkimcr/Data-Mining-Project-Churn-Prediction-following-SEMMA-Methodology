# -*- coding: utf-8 -*-
"""DM_project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MtCtoixVFTZ5ZlcEwzpGfvDdRmoRQ29c

# Data Mining Project
### Jiyoung Kim - Student ID 110075
### Anastasia Karuzina - Student ID 85536

**Target variable:** CHURN RATE

**Hypothesis 1:** Customers who pay for the service more money than on average has higher chance to stop using the service.

**Hypothesis 2:** Annual term contract customer has less channce to leave than monthly term contract customer.

**Hypothesis 3:** If a customer uses additional services of Telecom, she will not stop using the service.


**Hypothesis 4:** Internet service can affect the churn rate.

## **Step 1. Sample**
"""

# Commented out IPython magic to ensure Python compatibility.
# Import packages
import pandas as pd
import numpy as np
#import sklearn import preprocessing

## For Visualization
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
from pandas.plotting import scatter_matrix
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer
from sklearn_pandas import DataFrameMapper, cross_val_score
import datetime
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
# %matplotlib inline

# Set font size for plotting
sns.set(font_scale=1)

# Load dataset
telco_org = pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv", na_values=' ') 
# to prevent values with blank space being considered as value
telco_org.head()

# Drop the Customer ID column since we will not use
telco = telco_org.drop('customerID', axis=1)

telco_org.info()

# missing value check 
telco.isnull().sum() # 11 missing values in TotalCharges

telco.dropna(inplace=True)
telco.reset_index(drop=True, inplace=True)

telco.info()

"""In here, we can actually see a variables with wrong data type. 
*   SeniorCitizen should be an object (categorical variable)


"""

# Data type revision
telco["SeniorCitizen"] = telco["SeniorCitizen"].astype(str)
telco.dtypes

# continuous variables statistics
telco.describe()

# let's keep original dataset done with basic preprocessing
telco_copy = telco.copy()

# Plot - target var: Churn
churn_bar = sns.countplot(data=telco_copy, x="Churn", palette="Set2")
for p in churn_bar.patches:
    churn_bar.annotate(f'\n{p.get_height()}', (p.get_x()+0.4, p.get_height()), ha='center', va='top', color='white', size=18)

plt.show()

"""####**Conversion of Categorical variables (non-numeric inputs)**

**(Create Dummy columns & Text columns to Integers)**

1) Variables with 2 level YES/NO → 1/0

2) Variables with 3 levels YES/NO/No Internet Service (in this case, we will regard it as NO) → 1/0

3) Variables with several levels → create dummy columns
"""

telco.gender = [1 if each == "Male" else 0 for each in telco.gender]
telco.SeniorCitizen = [1 if each == "1" else 0 for each in telco.SeniorCitizen]

columns_to_change1 = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 
                       'TechSupport', 'StreamingTV', 'StreamingMovies']
        
for column in columns_to_change1:
   telco[column] = telco[column].replace({'No internet service':'No'})

columns_to_change2 = ['Partner','Dependents','PhoneService','OnlineSecurity',
                      'OnlineBackup','DeviceProtection','TechSupport',
                      'StreamingTV','StreamingMovies','PaperlessBilling','Churn']

for column in columns_to_change2:
    telco[column] = telco[column].map({'Yes':1, 'No':0})

dummy_col = ['MultipleLines', 'InternetService', 'Contract', 'PaymentMethod']
telco_new = pd.get_dummies(data=telco, columns = dummy_col)
telco_new.head()
    
telco.head()

# We still have some categorical(factor) variables are left such as InternetService, Contract, PaymentMethod.
# For these variables, we will create dummy columns.

dummy_col = ['MultipleLines', 'InternetService', 'Contract', 'PaymentMethod']
telco_new = pd.get_dummies(data=telco, columns = dummy_col)
telco_new.head()

telco_new.info()

"""#### **Data Partitioning**"""

# 1) Data Partitioning (70/30) - In default way of using sklearn (X-only independent variables, y-our target variables)
# Since our dataset is relatively small, we didn't split it into 3 sets (train, validation, test)

from sklearn.model_selection import train_test_split

X = telco_new.drop('Churn', axis=1) 
y = telco_new['Churn']

# Train - Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state = 42)

X_train_shape = X_train.shape
X_test_shape = X_test.shape
print("X_train shape = {}\nX_test shape = {}".format(X_train_shape, X_test_shape))

"""## **Step 2. Explore**

### **EDA (Multiplot in SAS Miner)**

Through this basic EDA part, we will check all variables' distribution and proportion of Churn(yes or no) in each of them. (we can also consider this EDA results when we decide how to deal with highly correlated variables(variables with multicollinearity) in our next step.

**Numeric Variables**
"""

sns.displot(telco, x='MonthlyCharges', hue='Churn', element='step')
sns.displot(telco, x='TotalCharges', hue='Churn', element='step')
sns.displot(telco, x='tenure', hue='Churn', element='step')

"""**Categorical Variables**"""

# Plot - How does distribution of all variables look like in overall
import math

columns = telco.columns.tolist()
num_var = ['tenure','MonthlyCharges','TotalCharges'] # exempt the numerical variables
for i in num_var:
    columns.remove(i)
    
f, axes = plt.subplots(math.ceil(len(columns)/4),4, figsize=(30,30))
for col, ax in zip(columns, axes.ravel()):
        data = telco[col].value_counts().values.tolist()
        labels = telco[col].value_counts().index.tolist()
        ax.pie(data,autopct='%0.2f%%')
        ax.set_title(col)
        ax.legend(labels,loc='best')

# MultiPlot - target var: Churn
columns_obj = telco.columns.tolist()
num_var = ['tenure','MonthlyCharges','TotalCharges'] # exempt the numerical variables # (task to do) also need to delete Customer_id
for i in num_var:
    columns_obj.remove(i)

fig, axs = plt.subplots(4, 4, sharex=False, sharey=True, figsize=(30,30))

count_row = 0
count_columns = 0
for column in columns_obj:    
    
    Bar_Plot = sns.countplot(x=column,hue='Churn',data=telco, palette="Set2", ax = axs[count_row][count_columns]).set_title("Frequeny distribution of Churn for " + str(column))
        
   
    count_columns +=1
    
    if count_columns == 4:
        count_row+=1
        count_columns=0

"""###**Correlation (StatExplore in SAS miner)**

Checking correlation between variables, drawing correlation matrix using Spearman's Rank Correlation.

##### Spearman correlation matrix for continuous variables: Total Charges, Tenure, Montly Charges
"""

cont_var = telco[["TotalCharges", "tenure", "MonthlyCharges"]]
corr = cont_var.corr(method = 'spearman') #correlation matrix for float and int 

corr_map = plt.figure(figsize=(6,6))
our_map = sns.heatmap (corr, xticklabels = corr.columns, cmap='coolwarm', yticklabels = corr.columns, linewidths =.75, annot = True)

"""As for continuous variablels, let's take a closer look at them."""

sns.pairplot(telco,vars = ['tenure','MonthlyCharges','TotalCharges'], hue="Churn")

"""People having lower tenure and higher monthly charges are tend to churn more.

#### Cramers V correlation matrix for categorical variables
"""

columns_all =[]
for column in telco.columns:
  if telco[column].nunique() <5:
    columns_all.append(column)

import scipy.stats as ss

df_corr = telco[columns_all]
# Cramers V for categorical correlations
def cramers_v(x, y):
    x = np.array(x)
    y = np.array(y)
    confusion_matrix = pd.crosstab(x, y)
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2/n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))
    rcorr = r-((r-1)**2)/(n-1)
    kcorr = k-((k-1)**2)/(n-1)
    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))

cramersv = pd.DataFrame(index=df_corr.columns,columns=df_corr.columns)
columns = df_corr.columns

for i in range(0,len(columns)):
    for j in range(0,len(columns)):
        #print(data[columns[i]].tolist())
        u = cramers_v(df_corr[columns[i]].tolist(),df_corr[columns[j]].tolist())
        cramersv.loc[columns[i],columns[j]] = u
        
cramersv.fillna(value=np.nan,inplace=True)

churn_only = cramersv[['Churn']]
new_churn = churn_only.sort_values(by=['Churn'])

plt.figure(figsize=(20,20))
ax = sns.heatmap(cramersv,cmap="coolwarm", annot = True )
plt.show()

"""
Checking the highest correlation with our target variable as code"""

plt.figure(figsize=(6,10))
sns.heatmap(new_churn, cmap="coolwarm", annot=True, cbar = False)

"""**Findings:**

1) Total Charges and Tenure are highly correlated (0.89). \
→ Considering that TotalCharges can be derived from tenure*Monthly Charges, we can just drop the Total Charges.  


2) Streaming TV, Streaming Movies services are quite correlated between each other (0.53)  \
→ Considering the EDA results, the distribution of values (1/0 and also churn distribution) of Streaming TV and Movies were almost same. So, we decided to assemble those two and make it into one variable 'Streaming Services'.


3) PhoneService and Multiple lines are very correlated between each other (correlation = 1) \
→ We will drop PhoneService (Bcz it's less correlated with our Target Variable Churn than the Multiple lines). 

4) Contract is 41% correlated with Churn. We should take it into consideration and investigate in later steps.  
"""

# Drop some variables based on the result of Correlation matrix
telco_new = telco_new.drop(['TotalCharges', 'PhoneService'], axis=1)

# Create new column 'Streaming' 
telco_new.loc[(telco_new['StreamingTV'] + telco_new['StreamingMovies']) >= 1,'Streaming'] = 1
telco_new['Streaming'].fillna(0, inplace=True)

telco_new = telco_new.drop(['StreamingTV', 'StreamingMovies'], axis=1)

# Data type revision
telco_new["Streaming"] = telco_new["Streaming"].astype(int)
telco_new.dtypes

# Do our data partitioning one more time with new dataset
# 1) Data Partitioning (70/30) - In default way of using sklearn (X-only independent variables, y-our target variables)
# Since our dataset is relatively small, we didn't split it into 3 sets (train, validation, test)
# Instead, we will use K-fold Cross validation later.

from sklearn.model_selection import train_test_split

X = telco_new.drop('Churn', axis=1) 
y = telco_new['Churn']

# Train - Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state = 42)

X_train_shape = X_train.shape
X_test_shape = X_test.shape
print("X_train shape = {}\nX_test shape = {}".format(X_train_shape, X_test_shape))

"""### **Variable Selection**

Based on the test results we can eliminate those variables which are not strongly associated with the response variable. Alternatively, we can also check the association of independent variables among themselves and can drop those variables which are strongly associated with each other.

The Chi-square test of Independence determines whether there is an association between two categorical variables i.e. whether the variables are independent or related like for example if education level and marital status are related for all people in some country.

Variable selection part.

The null hypothesis (H0) and alternative hypothesis (H1) of the **Chi-Square test of Independence** can be expressed like below

H0: “[Churn] is independent of [Variable]”

H1: “[Churn] is not independent of [Variable]”

We are using **α = 0.05,** that would be 95% confidence interval. Based on above heat map we can conclude following inference

#### **Method 1> Univariate Statistical Tests**
**(Chi-squared Test of Independence)**

We can use many different statistical test like Chi-square, ANOVA F-value test for this Univariate Selection method. Among them, we will use chisquare method using chi2() function from sckitlearn.
"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import plotly.offline as py
import plotly.figure_factory as ff

# fit model with k=14
test = SelectKBest(score_func=chi2, k = 14)
fit = test.fit(X_train, y_train)

# create result dataframe
score = pd.DataFrame({"Variable": X_train.columns, "Score": fit.scores_, "p_value": fit.pvalues_ })
score = score.sort_values(by = "Score", ascending=False)

table_score = ff.create_table(score)
py.iplot(table_score)

# if we want to only leave the best 14 variables
X_best_new = test.fit_transform(X_train, y_train)

mask = fit.get_support() #list of booleans for selected features
new_feat = [] 
for bool, feature in zip(mask, X_train.columns):
 if bool:
   new_feat.append(feature)
print('The best features are:{}'.format(new_feat)) # The list of your 10 best features
print('Original number of features:', X_train.shape)
print('Reduced number of features:', X_best_new.shape)

# Create new dataset considering this Variable Selection
selected_features = pd.DataFrame(test.inverse_transform(X_best_new), 
                                 index=X_train.index, 
                                 columns=X_train.columns)

# Dropped columns with values of all 0s, so var is 0, drop them
selected_columns = selected_features.columns[selected_features.var() != 0]

selected_columns

# Create new dataframe with selected variables
X_train_selected = X_train[selected_columns]
X_test_selected = X_test[selected_columns]

X_train_selected

"""#### **Method 2> Feature Importance Method (using RandomForestClassifier)** 

As we use decision trees (random forest), we can select variables based on high importance.
"""

# Feature importance is inbuilt with Tree Based Classifiers
from sklearn.ensemble import RandomForestClassifier
np.random.seed(42)

# We already made splited X (independent X columns) and y (target column) in Data Partitioning Step, so we will use them.
# instantiate RandomForestClassifier
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
feat_importances = pd.Series(rf_model.feature_importances_, index=X.columns)

# determine 14 most important features
df_imp_feat = feat_importances.nlargest(14)

# plot 14 most important features
df_imp_feat.plot(kind='barh').invert_yaxis()
plt.show()
print(df_imp_feat)

"""#### **Method 3> Feature Correlation Method** 


"""

# determine 14 most correlated features
telco_new_corr_churn = telco_new.corr()['Churn'].sort_values(ascending=False).head(15)
top_corr_features = telco_new_corr_churn.index
# plot top 14 most correlated features to our target (Churn)
telco_new_corr_churn.plot(kind='barh').invert_yaxis()
plt.show()
# export selected features to .csv
# telco_new_corr_churn.to_csv('feature_selection_CORRELATION.csv')
print(telco_new_corr_churn)

"""Above method 3 result tells us that Variables such as Month-to-Month, using Internet Service with 'Fiber optic' and Electronic check payment method looks like positively correlated with our target 'Churn'. On the other hand, the variables such as Device Protection, not using Multuiple Lines or phone service show negatively correlated with Churn, which means they have less correlation with our target.

###**Feature Scaling - using Standardization**

Before we proceed the Clustering, it's necessary to do a feature scaling for the numerical(continuous) variables in our dataset since each numeric variable has different standard of measure. There are several methods to proceed Feature Scaling such as normalization, standardization, etc. Among them, we will do Standardization using StandardScaler function. It makes each numerical feature to have mean '0', std '1' while keeping the shape of the data.
"""

#Feature Scaling for continuous(num) variables before Clustering

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train_selected[['tenure','MonthlyCharges']])
X_train_selected[['tenure','MonthlyCharges']] = sc.transform(
    X_train_selected[['tenure','MonthlyCharges']])

X_train_selected[['tenure','MonthlyCharges']].describe()

# sc.fit(X_test_selected[['tenure','MonthlyCharges','TotalCharges']]) 
# since we already fit with our train data, test dataset doesn't need to fit again.
X_test_selected[['tenure','MonthlyCharges']] = sc.transform(X_test_selected[['tenure','MonthlyCharges']])

"""###**Variable Clustering**

To follow Clustering method operated in SAS, we will use VarClusHi module  in python to perform variable clustering with keeping a hierarchical structure. Through using this, we are expecting to reduce our dimension by selecting 'Best Variables' based on R-square with Own Cluster Component and R-squared ratio.
"""

!pip install varclushi

from varclushi import VarClusHi

#or should I use only X_train set as input?
cluster_result = VarClusHi(X_train_selected, maxeigval2 = 0.7, maxclus = None)
cluster_result.varclus()

# Number of Clusters, Number of Variables in each Cluster(N_vars), variance explained by each cluster (Eigval1), etc.
cluster_result.info

# R-square ratio of variables in the clusters 
# (R-Square with Own Cluster Component, R-Square with next cluster component, 1-RSquared Ratio)
cluster_result.rsquare.sort_values(by=["Cluster", "RS_Ratio"], ascending = (True, True))

# From this result, we will only select the best variables from each cluster
X_train_clus_selected = X_train_selected[['Contract_Two year', 'MonthlyCharges', 'Contract_One year', 'PaymentMethod_Credit card (automatic)',
        'OnlineSecurity', 'Dependents', 'SeniorCitizen', 'PaperlessBilling']]

X_test_clus_selected = X_test_selected[['Contract_Two year', 'MonthlyCharges', 'Contract_One year', 'PaymentMethod_Credit card (automatic)',
        'OnlineSecurity', 'Dependents', 'SeniorCitizen', 'PaperlessBilling']]

X_train_clus_selected.info()

X_test_clus_selected.info()

"""##**Step 3. Modify**

1. Grouping the similar/highly correlated variables considering distribution
2. Grouping Tenure variable into 3 groups

##**Step 4. Model**

####**Logistic Regression**
We will build 3 different Logistic Regression Model: 

1) Model with full range of variables

2) Model with Selected Variables

*   2-1) Model with Selected variables using RFE (recursive feature elimination) 
*   2-2) Model with Selected variables from Previous Explore Step (Univariate selection with Chi-square test + Variable Clustering)
"""

# !pip uninstall scikit-learn -y
!pip install -U scikit-learn

"""##### **1) Logistic Regression Modelling with Full Variables**"""

# Feature Scaling (standardization) for our original train, test data (even before variable selection)
# Feature Scaling is also essential for machinelearning algorithms to have proper insight and interpretation from the result.

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train[['tenure','MonthlyCharges']])
X_train[['tenure','MonthlyCharges']] = sc.transform(X_train[['tenure','MonthlyCharges']])

X_train.describe()
sc.fit(X_test[['tenure','MonthlyCharges']])
X_test[['tenure','MonthlyCharges']] = sc.transform(X_test[['tenure','MonthlyCharges']])

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(C = 1e6, solver='lbfgs', max_iter=15000, random_state = 42) # add C = 1e6 for not doing a regularization
lr_model.fit(X_train, y_train)
lr_y_pred = lr_model.predict(X_test)
lr_y_pred_train = lr_model.predict(X_train)

from sklearn import metrics
# Model Evaluation using Confusion Matrix
cnf_matrix = metrics.confusion_matrix(y_test, lr_y_pred)

# Visualization of Confusion Matrix
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("R²=", lr_model.score(X_train, y_train))
print("intercept = ", lr_model.intercept_)

"""In this step we are printing all coefficient of each variable."""

for f, w in zip(X_train.columns, lr_model.coef_[0]):
  print(f"{f:<30} {w}")

"""To sum up the result of the logistic regression we import necessary additions from sklearn to add confusion matrix evaluation metrics. We will further use it for model comparison."""

from sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score, confusion_matrix

test_acc = metrics.accuracy_score(lr_y_pred, y_test)
train_acc = metrics.accuracy_score(lr_y_pred_train, y_train)
prec = metrics.precision_score(lr_y_pred, y_test)
rec = metrics.recall_score(lr_y_pred, y_test)
f1 = f1_score(lr_y_pred, y_test)

results = pd.DataFrame([['Logistic Regression (Base Model)', test_acc, train_acc, prec, rec, f1]],columns=['Model', 'Test Accuracy', 'Train Accuracy', 
                                                                                                           'Precision', 'Recall', 'F1 Score'])
results

from sklearn.metrics import classification_report
print(classification_report(lr_y_pred, y_test))

"""As we can see the model does a good job with prediction having 80% accuracy."""

# ROC Curve
lr_predictions_proba = lr_model.predict_proba(X_test)[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, lr_predictions_proba)

auc = metrics.roc_auc_score(y_test, lr_predictions_proba) 

plt.plot(fpr,tpr,label="LR (AUC="+str(auc)+")")
plt.title('Receiver operating characteristic(ROC) Curve')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.legend(loc=4)
plt.plot([0, 1], [0, 1],linestyle='--')
plt.show()

# AUC Score for the case is 0.83 (If AUC score is closer to 1, it means perfect classifier, and closer to 0.5 means a worthless classifier)
y_proba_log_train = lr_model.predict_proba(X_train)[:, 1]
auc_train = metrics.roc_auc_score(y_train, y_proba_log_train)

print(f"AUC score for test data: {auc}")
print(f"AUC score for train data: {auc_train}")

# !pip install scikit-plot # <- if required

# pip install delayed  # <- if required

import scikitplot as skplt

pred1 = lr_model.predict_proba(X_test)
# pred2 = model2.predict_proba(test_x)

plt.figure(figsize=(7,7))
skplt.metrics.plot_cumulative_gain(y_test, pred1)
plt.show()

"""**[For interpretation] Detailed summary of Logistic Regression full model result**"""

import statsmodels.api as sm
glm = sm.GLM(y_train, X_train, family = sm.families.Binomial()).fit()
glm.summary()

# Odds Ratio
np.exp(glm.params)

"""Droping(rejecting) the variables with p-value >0.05 and re-do LR model fitting"""

# Variable filtering based on p-value
X_train_new = X_train.drop(['gender', 'SeniorCitizen', 'Partner', 'DeviceProtection', 'InternetService_Fiber optic',
                            'InternetService_No', 'PaymentMethod_Electronic check', 
                            'PaymentMethod_Mailed check', 'Streaming'], axis=1)
X_test_new = X_test.drop(['gender', 'SeniorCitizen', 'Partner', 'DeviceProtection', 'InternetService_Fiber optic',
                            'InternetService_No', 'PaymentMethod_Electronic check', 
                            'PaymentMethod_Mailed check', 'Streaming'], axis=1)

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(C = 1e6, solver='lbfgs', max_iter=15000, random_state = 42) # add C = 1e6 for not doing a regularization
lr_model.fit(X_train_new, y_train)
lr_y_pred = lr_model.predict(X_test_new)
lr_y_pred_train = lr_model.predict(X_train_new)

from sklearn import metrics
# Model Evaluation using Confusion Matrix
cnf_matrix = metrics.confusion_matrix(y_test, lr_y_pred)

# Visualization of Confusion Matrix
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("R²=", lr_model.score(X_train_new, y_train))
print("intercept = ", lr_model.intercept_)

# ROC Curve
lr_predictions_proba = lr_model.predict_proba(X_test_new)[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, lr_predictions_proba)

auc = metrics.roc_auc_score(y_test, lr_predictions_proba) 

plt.plot(fpr,tpr,label="LR (AUC="+str(auc)+")")
plt.title('Receiver operating characteristic(ROC) Curve')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.legend(loc=4)
plt.plot([0, 1], [0, 1],linestyle='--')
plt.show()

# AUC Score for the case is 0.83 (If AUC score is closer to 1, it means perfect classifier, and closer to 0.5 means a worthless classifier)
y_proba_log_train = lr_model.predict_proba(X_train_new)[:, 1]
auc_train = metrics.roc_auc_score(y_train, y_proba_log_train)

print(f"AUC score for test data: {auc}")
print(f"AUC score for train data: {auc_train}")

from sklearn.metrics import classification_report
print(classification_report(lr_y_pred, y_test))

# Confusion Matrix Evaluation Metrics
from sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score, confusion_matrix

test_acc = metrics.accuracy_score(lr_y_pred, y_test)
train_acc = metrics.accuracy_score(lr_y_pred_train, y_train)
prec = metrics.precision_score(lr_y_pred, y_test)
rec = metrics.recall_score(lr_y_pred, y_test)
f1 = f1_score(lr_y_pred, y_test)

results0 = pd.DataFrame([['Logistic Regression (var select with p-value)', test_acc, train_acc, prec, rec, f1]],columns=['Model', 'Test Accuracy', 'Train Accuracy', 
                                                                                                           'Precision', 'Recall', 'F1 Score'])
results0

import statsmodels.api as sm
glm = sm.GLM(y_train, X_train_new, family = sm.families.Binomial()).fit()
glm.summary()

# Odds Ratio
np.exp(glm.params)

"""##### **2) Logistic Regression Modelling with Selected Variables**

In Python, we don't have option for Stepwise selection but only Forward or Backward selection for variable selection (ex. Sequential Feature Selector from sci-kit learn). So, instead of just using ordinary fw, bw method, we decided to use the RFE which means recursive feature eliminiation. This one is very similar to Backward regression, but still a bit more advanced in that this approach does the whole cycle of eliminations and then chooses the best subset of it while ordinary Backward method stops at the point where the score starts decreasing. 

Recursive feature elimination is based on the idea to repeatedly construct a model (for example an SVM or a regression model) and choose either the best or worst performing feature (for example based on coefficients), setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. Features are then ranked according to when they were eliminated. As such, it is a greedy optimization for finding the best performing subset of features.

###### **2-1) Variable Selection using RFE (recursive feature elimination)**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn import preprocessing
from sklearn.feature_selection import RFE

# Feature extraction
lr2_model = LogisticRegression(solver='lbfgs', max_iter=15000)
rfe = RFE(lr2_model, n_features_to_select=None) #None means just half of the features are selected
fit = rfe.fit(X_train, y_train)
print("Num Features: %s" % (fit.n_features_))
print("Selected Features: %s" % (fit.support_))
print("Feature Ranking: %s" % (fit.ranking_))

selected_features_rfe = list(fit.support_)
final_features_rfe = []    
for status, var in zip(selected_features_rfe, X_train.columns):
    if status == True:
        final_features_rfe.append(var)
        
final_features_rfe

X_train_rfe = X_train[['Dependents', 'tenure', 'OnlineSecurity', 'OnlineBackup',
 'TechSupport', 'PaperlessBilling', 'MonthlyCharges', 'MultipleLines_No',
 'MultipleLines_No phone service', 'Contract_Month-to-month', 'Contract_Two year',
 'PaymentMethod_Electronic check']]

X_test_rfe = X_test[['Dependents', 'tenure', 'OnlineSecurity', 'OnlineBackup',
 'TechSupport', 'PaperlessBilling', 'MonthlyCharges', 'MultipleLines_No',
 'MultipleLines_No phone service', 'Contract_Month-to-month', 'Contract_Two year',
 'PaymentMethod_Electronic check']]

"""Above table shows that RFE filtered variables well because p-value of all features is < 0.05"""

# Logistic Regression Model with selected features from RFE

lr2_model = LogisticRegression(C = 1e6, solver='lbfgs', max_iter=15000, random_state = 42) # add C = 1e6 for not doing a regularization

lr2_model.fit(X_train_rfe, y_train)
lr2_y_pred = lr2_model.predict(X_test_rfe)
lr2_y_pred_train = lr2_model.predict(X_train_rfe)

print("R²=", lr2_model.score(X_train_rfe, y_train))
print("intercept = ", lr2_model.intercept_)

# Print coefficient of each variable
for f, w in zip(X_train_rfe.columns, lr2_model.coef_[0]):
  print(f"{f:<30} {w}")

from sklearn import metrics
# Model Evaluation using Confusion Matrix
cnf_matrix = metrics.confusion_matrix(y_test, lr2_y_pred)

# Visualization of Confusion Matrix
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

# Confusion Matrix Evaluation Metrics
from sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score, confusion_matrix

test_acc = metrics.accuracy_score(lr2_y_pred, y_test)
train_acc = metrics.accuracy_score(lr2_y_pred_train, y_train)
prec = metrics.precision_score(lr2_y_pred, y_test)
rec = metrics.recall_score(lr2_y_pred, y_test)
f1 = f1_score(lr2_y_pred, y_test)

results2 = pd.DataFrame([['Logistic Regression (selected var with RFE)', test_acc, train_acc, prec, rec, f1]],columns=['Model', 'Test Accuracy', 'Train Accuracy', 
                                                                                                           'Precision', 'Recall', 'F1 Score'])
results2

from sklearn.metrics import classification_report
print(classification_report(lr2_y_pred, y_test))

# ROC Curve
lr2_predictions_proba = lr2_model.predict_proba(X_test_rfe)[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, lr2_predictions_proba)

auc2 = metrics.roc_auc_score(y_test, lr2_predictions_proba) 

plt.plot(fpr,tpr,label="Logistic Reg (AUC="+str(auc2)+")")
plt.title('Receiver operating characteristic(ROC) Curve')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.legend(loc=4)
plt.plot([0, 1], [0, 1],linestyle='--')
plt.show()

# AUC Score for the case is 0.84 (If AUC score is closer to 1, it means perfect classifier, and closer to 0.5 means a worthless classifier)
y_proba_log_train_2 = lr2_model.predict_proba(X_train_rfe)[:, 1]
auc2_train = metrics.roc_auc_score(y_train, y_proba_log_train_2)

print(f"AUC score for test data: {auc2}")
print(f"AUC score for train data: {auc2_train}")

import scikitplot as skplt

# pred1 = clf.predict_proba(X_test)
pred_lr2 = lr2_model.predict_proba(X_test_rfe)

plt.figure(figsize=(7,7))
skplt.metrics.plot_cumulative_gain(y_test, pred_lr2)
plt.show()

import statsmodels.api as sm
glm2 = sm.GLM(y_train, X_train_rfe, family = sm.families.Binomial()).fit()
glm2.summary()

# Odds Ratio
np.exp(glm2.params)

"""###### **2-2) LR Model with Variable Selection with Univariate + Clustering**"""

from sklearn.linear_model import LogisticRegression

lr3_model = LogisticRegression(solver='lbfgs', max_iter=1000)
lr3_model.fit(X_train_clus_selected, y_train)
lr3_y_pred = lr3_model.predict(X_test_clus_selected)
lr3_y_pred_train = lr3_model.predict(X_train_clus_selected)

print("R²=", lr3_model.score(X_train_clus_selected, y_train))
print("intercept = ", lr3_model.intercept_)

from sklearn import metrics
# Model Evaluation using Confusion Matrix
cnf_matrix = metrics.confusion_matrix(y_test, lr3_y_pred)

# Visualization of Confusion Matrix
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

# Confusion Matrix Evaluation Metrics
from sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score, confusion_matrix

test_acc = metrics.accuracy_score(lr3_y_pred, y_test)
train_acc = metrics.accuracy_score(lr3_y_pred_train, y_train)
prec = metrics.precision_score(lr3_y_pred, y_test)
rec = metrics.recall_score(lr3_y_pred, y_test)
f1 = f1_score(lr3_y_pred, y_test)

results3 = pd.DataFrame([['Logistic Regression (Univariate+Clustering)', test_acc, train_acc, prec, rec, f1]],columns=['Model', 'Test Accuracy', 'Train Accuracy', 
                                                                                                           'Precision', 'Recall', 'F1 Score'])
results3

from sklearn.metrics import classification_report
print(classification_report(lr3_y_pred, y_test))

# ROC Curve
lr3_predictions_proba = lr3_model.predict_proba(X_test_clus_selected)[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, lr3_predictions_proba)

auc3 = metrics.roc_auc_score(y_test, lr3_predictions_proba) 

plt.plot(fpr,tpr,label="Logistic Reg (AUC="+str(auc3)+")")
plt.title('Receiver operating characteristic(ROC) Curve')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.legend(loc=4)
plt.plot([0, 1], [0, 1],linestyle='--')
plt.show()

# AUC Score for the case is 0.80 (If AUC score is closer to 1, it means perfect classifier, and closer to 0.5 means a worthless classifier)
y_proba_log_train_3 = lr3_model.predict_proba(X_train_clus_selected)[:, 1]
auc3_train = metrics.roc_auc_score(y_train, y_proba_log_train_3)

print(f"AUC score for test data: {auc3}")
print(f"AUC score for train data: {auc3_train}")

pred_lr3 = lr3_model.predict_proba(X_test_clus_selected)

plt.figure(figsize=(7,7))
skplt.metrics.plot_cumulative_gain(y_test, pred_lr3)
plt.show()

# LR ROC Curve Summary
fpr, tpr, thresholds = metrics.roc_curve(y_test, lr_predictions_proba) # Full model
fpr1, tpr1, thresholds = metrics.roc_curve(y_test, lr2_predictions_proba) # variable selected RFE model
fpr2, tpr2, thresholds = metrics.roc_curve(y_test, lr3_predictions_proba) # variable selected model

auc = metrics.roc_auc_score(y_test, lr_predictions_proba)
auc2 = metrics.roc_auc_score(y_test, lr2_predictions_proba)  
auc3 = metrics.roc_auc_score(y_test, lr3_predictions_proba) 

import matplotlib.pyplot as plt
plt.figure(figsize=(10,10))
plt.title('ROC Curve (Logistic Regression Models)')
plt.plot(fpr,tpr, color='olivedrab',label = ['AUC = %0.3f' % auc,'Full Var'])
plt.plot(fpr1,tpr1, color='darkorange',label = ['AUC = %0.3f' % auc2,'RFE Var'])
plt.plot(fpr2,tpr2, color='mediumorchid',label = ['AUC = %0.3f' % auc3,'Univariate+Clustering Var'])

plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],linestyle='--')
plt.axis('tight')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

"""####**Decision Tree**

We will build 3 different DECISION TREES:

1) Model with full range of variables

2) Model with Selected Variables

2-1) Model with Selected variables using RFE (recursive feature elimination)

2-2) Model with Selected variables from Previous Explore Step (Univariate selection with Chi-square test + Variable Clustering)

##### 1) Model with full range of variables

1) all variables included => hard to interpret!
"""

import pandas as pd
from sklearn import tree
import pydotplus
from sklearn.tree import DecisionTreeClassifier #Decision Tree Classifier is imported
from sklearn import metrics

# in case if u didn't run the previous LR, and only run DT model part, need to run this scaling again

# Feature Scaling (standardization) for our original train, test data (even before variable selection)
# Feature Scaling is also essential for machinelearning algorithms to have proper insight and interpretation from the result.

# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()
# sc.fit(X_train[['tenure','MonthlyCharges']])
# X_train[['tenure','MonthlyCharges']] = sc.transform(X_train[['tenure','MonthlyCharges']])

# X_train.describe()
# sc.fit(X_test[['tenure','MonthlyCharges']])
# X_test[['tenure','MonthlyCharges']] = sc.transform(X_test[['tenure','MonthlyCharges']])

#Create Decision Tree classifier object 
clf = DecisionTreeClassifier()

#Train Decision Tree Classifier

clf = clf.fit(X_train, y_train)

# Predict the response for test set
Churn_train = clf.predict(X_train)
Churn_predict = clf.predict(X_test)

#Plotting report and ROC
print(classification_report(y_test,Churn_predict))
metrics.plot_roc_curve(clf, X_test, y_test)

# new version
dt_predictions_proba = clf.predict_proba(X_test)[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, dt_predictions_proba)
auc = metrics.roc_auc_score(y_test, dt_predictions_proba) 

plt.plot(fpr,tpr,label="DT(Full) (AUC="+str(auc)+")")
plt.title('Receiver operating characteristic(ROC) Curve')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.legend(loc=4)
plt.plot([0, 1], [0, 1],linestyle='--')
plt.show()

# AUC Score for the case is 0.83 (If AUC score is closer to 1, it means perfect classifier, and closer to 0.5 means a worthless classifier)
y_proba_dt_train = clf.predict_proba(X_train)[:, 1]
auc_train = metrics.roc_auc_score(y_train, y_proba_dt_train)

print(f"AUC score for test data: {auc}")
print(f"AUC score for train data: {auc_train}")

import scikitplot as skplt

pred1 = clf.predict_proba(X_test)
# pred2 = model2.predict_proba(test_x)

plt.figure(figsize=(7,7))
skplt.metrics.plot_cumulative_gain(y_test, pred1)
plt.show()

"""Checked importance of variables based on Deicison Tree Classifier"""

feat_importances_tree = pd.Series(clf.feature_importances_, index=X_train.columns)

# determine 14 most important features
df_imp_feat_tree = feat_importances_tree.nlargest(14)

# plot 14 most important features
df_imp_feat_tree.plot(kind='barh')
plt.show()
print(df_imp_feat_tree)

"""Accuracy can be computed by comparing actual test set values and predicted values"""

cnf_matrix_tree_all = metrics.confusion_matrix(y_test, Churn_predict)

# Visualization of Confusion Matrix
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix_tree_all), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("Accuracy:", metrics.accuracy_score(y_test, Churn_predict))

# Confusion Matrix Evaluation Metrics
# Churn_train = clf.predict(X_train)
# Churn_predict = clf.predict(X_test)

test_acc = metrics.accuracy_score(Churn_predict, y_test)
train_acc = metrics.accuracy_score(Churn_train, y_train)
prec = metrics.precision_score(Churn_predict, y_test)
rec = metrics.recall_score(Churn_predict, y_test)
f1 = f1_score(Churn_predict, y_test)

results_DT1 = pd.DataFrame([['Decision Tree (Base Model: full set of variables)', test_acc, train_acc, prec, rec, f1]],columns=['Model', 'Test Accuracy', 'Train Accuracy', 
                                                                                                           'Precision', 'Recall', 'F1 Score'])
results_DT1

"""A classification rate of 71.8%, considered as good accuracy. We can improve this accuracy by tuning the parameters in the Decision Tree Algorithm."""

pip install graphviz #if required

pip install pydotplus #if required

from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus

"""In the decision tree chart, each internal node has a decision rule that splits the data. Gini referred as Gini ratio, which measures the impurity of the node. You can say a node is pure when all of its records belong to the same class, such nodes known as the leaf node.

Here, the resultant tree is unpruned. This unpruned tree is unexplainable and not easy to understand. In the next section, let's optimize it by pruning.
"""

dot_data = StringIO()
features = X_train.columns
export_graphviz(clf, out_file = dot_data,
                filled = True, rounded = True, special_characters = True, feature_names = features, class_names = ['0', '1'] )
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('tree.png')
Image(graph.create_png())

"""We need to get rid of unimportant variables. Let's do prunning, check the model accurace and see what variables are important.

##### **2) Pruning our Decision tree**
"""

clf_prun = DecisionTreeClassifier(criterion = "gini", max_depth=3)
#entropy = Information Gain
#can change to gini - explanation of differences -> link ------->
clf_prun = clf_prun.fit(X_train, y_train)

Churn_predict_prun = clf_prun.predict(X_test)
Churn_predict_clf_train = clf_prun.predict(X_train)

print("Accuracy:", metrics.accuracy_score(y_test, Churn_predict_prun))

# Confusion Matrix Evaluation Metrics
# Churn_train = clf_prub.predict(X_train)
# Churn_predict_prun = clf_prun.predict(X_test)

test_acc = metrics.accuracy_score(Churn_predict_prun, y_test)
train_acc = metrics.accuracy_score(Churn_predict_clf_train, y_train)
prec = metrics.precision_score(Churn_predict_prun, y_test)
rec = metrics.recall_score(Churn_predict_prun, y_test)
f1 = f1_score(Churn_predict_prun, y_test)

results_DT2 = pd.DataFrame([['Decision Tree (Prunned model using Gini criterion)', test_acc, train_acc, prec, rec, f1]],columns=['Model', 'Test Accuracy', 'Train Accuracy', 
                                                                                                           'Precision', 'Recall', 'F1 Score'])
results_DT2

"""The classification rate increased  to 78.48%, which is better accuracy than the previous model. """

print(classification_report(y_test,Churn_predict_prun))
metrics.plot_roc_curve(clf_prun, X_test, y_test)

# new version
dt2_predictions_proba = clf_prun.predict_proba(X_test)[:,1]
fpr2, tpr2, threshold = metrics.roc_curve(y_test, dt2_predictions_proba)
auc2 = metrics.roc_auc_score(y_test, dt2_predictions_proba) 

plt.plot(fpr2,tpr2,label="DT(Prunned) (AUC="+str(auc2)+")")
plt.title('Receiver operating characteristic(ROC) Curve')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.legend(loc=4)
plt.plot([0, 1], [0, 1],linestyle='--')
plt.show()

# AUC Score for the case is 0.83 (If AUC score is closer to 1, it means perfect classifier, and closer to 0.5 means a worthless classifier)
y_proba_dt_train = clf_prun.predict_proba(X_train)[:, 1]
auc_train = metrics.roc_auc_score(y_train, y_proba_dt_train)

print(f"AUC score for test data: {auc2}")
print(f"AUC score for train data: {auc_train}")

import scikitplot as skplt

# pred1 = clf.predict_proba(X_test)
pred2 = clf_prun.predict_proba(X_test)

plt.figure(figsize=(7,7))
skplt.metrics.plot_cumulative_gain(y_test, pred2)
plt.show()

"""Cumulative Gaains Curve, according to it, if we approach 40% of our customer base (x-axis), we will get over almost 80% pf people who will leave the company(class 1).

Importantce of prunned tree variables
"""

feat_importances_prun_tree = pd.Series(clf_prun.feature_importances_, index=X_train.columns)

# determine most important features
df_imp_feat_prun_tree = feat_importances_prun_tree.nlargest(6)

# plot most important features
df_imp_feat_prun_tree.plot(kind='barh').invert_yaxis()
plt.show()
print(df_imp_feat_prun_tree)

# pip install graphviz

# Do our data partitioning one more time with new dataset (After correlation)
# 1) Data Partitioning (70/30)

from sklearn.model_selection import train_test_split

X = telco_new.drop('Churn', axis=1) 
y = telco_new['Churn']

# Train - Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state = 42)

X_train_shape = X_train.shape
X_test_shape = X_test.shape
print("X_train shape = {}\nX_test shape = {}".format(X_train_shape, X_test_shape))

from io import StringIO

dot_data1 = StringIO()
features = X.columns
export_graphviz(clf_prun, out_file = dot_data1,
                filled = True, rounded = True, special_characters = True, feature_names = features, class_names = ['0', '1'] )
graph = pydotplus.graph_from_dot_data(dot_data1.getvalue())
graph.write_png('tree.png')

Image(graph.create_png())

"""Contract_month-to-month <= 0.5 means that the customer with a contract value of less than 0.5 will follow the arrow to the left (which means everyone without monthly contract), and the rest will follow the arrow to the right.

Gini = 0.38 refers to a quality of split. it's always a number between 0.0 annd 0.5, where 0.0 would mean all of the samples got the same result, and 0.5 would mean that the split is done exactly in the middle.

Sample means that there are 4922 customers left at this point in the decision, which is all of them since this is the first step.

Value [3614, 1308]  means that of those customers, 36134 will get a churn as "NO" = 0, and 1308 will get a "YES" = 1.

Another way to look st decision tree
"""

pip install dtreeviz

from dtreeviz.trees import dtreeviz 

viz = dtreeviz(clf_prun, X, y,
                target_name="Churn", feature_names=X.columns)

viz

"""##### 3) Model with Selected Variables

Model with Selected variables from Previous Explore Step (Univariate selection with Chi-square test + Variable Clustering)

**Var selected from Univariate Chi + Var Clustered**
"""

#X_train_clus_selected

#Create Decision Tree classifier object

clf_clus = DecisionTreeClassifier()
#Train Decision Tree Classifier

clf_clus = clf_clus.fit(X_train_clus_selected, y_train)

# Predict the response for test set
Churn_predict_clus = clf_clus.predict(X_test_clus_selected)
Churn_train_clus = clf_clus.predict(X_train_clus_selected)

#Plotting report and ROC
print(classification_report(y_test,Churn_predict_clus))
metrics.plot_roc_curve(clf_clus, X_test_clus_selected, y_test)

print("Accuracy Chi+Clustered :", metrics.accuracy_score(y_test, Churn_predict_clus))

# new version ROC
dt3_predictions_proba = clf_clus.predict_proba(X_test_clus_selected)[:,1]
fpr3, tpr3, threshold = metrics.roc_curve(y_test, dt3_predictions_proba)
auc3 = metrics.roc_auc_score(y_test, dt3_predictions_proba) 

plt.plot(fpr3,tpr3,label="DT(Chi+Clustered Var)) (AUC="+str(auc3)+")")
plt.title('Receiver operating characteristic(ROC) Curve')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.legend(loc=4)
plt.plot([0, 1], [0, 1],linestyle='--')
plt.show()


y_proba_dt_train = clf_clus.predict_proba(X_train_clus_selected)[:, 1]
auc_train = metrics.roc_auc_score(y_train, y_proba_dt_train)

print(f"AUC score for test data: {auc3}")
print(f"AUC score for train data: {auc_train}")

# Confusion Matrix Evaluation Metrics
# Churn_predict_clus = clf_clus.predict(X_test_clus_selected)
# Churn_train_clus = clf_clus.predict(X_train_clus_selected)

test_acc = metrics.accuracy_score(Churn_predict_clus, y_test)
train_acc = metrics.accuracy_score(Churn_train_clus, y_train)
prec = metrics.precision_score(Churn_predict_clus, y_test)
rec = metrics.recall_score(Churn_predict_clus, y_test)
f1 = f1_score(Churn_predict_clus, y_test)

results_DT5 = pd.DataFrame([['Decision Tree (Chi+Clustered variables)', test_acc, train_acc, prec, rec, f1]],columns=['Model', 'Test Accuracy', 'Train Accuracy', 
                                                                                                           'Precision', 'Recall', 'F1 Score'])
results_DT5

# pred1 = clf.predict_proba(X_test)
pred3 = clf_clus.predict_proba(X_test_clus_selected)

plt.figure(figsize=(7,7))
skplt.metrics.plot_cumulative_gain(y_test, pred3)
plt.show()

Churn_predict_clus = metrics.confusion_matrix(y_test, Churn_predict_clus)

# Visualization of Confusion Matrix
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(Churn_predict_clus), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""#### XGBoost"""

# Xgboost
import xgboost as xgb


# create object model
Xgboost_model = xgb.XGBClassifier(objective = 'binary:logistic', 
                                  learning_rate=0.01,verbosity=1)

# fit the model
Xgboost_model.fit(X_train ,y_train)

# model score
predict_train = Xgboost_model.predict(X_train)
predict_test = Xgboost_model.predict(X_test)

# accuracy score
Xgb_train_score = Xgboost_model.score(X_train,y_train)
Xgb_test_score = Xgboost_model.score(X_test,y_test)

Xgb_f1_score = metrics.f1_score(predict_test, y_test)

print('Accuracy on Train set',Xgb_train_score)
print('Accuracy on Test set',Xgb_test_score)
print('F1-score on Test set:',Xgb_f1_score)
print('\n')
print(metrics.classification_report(y_test,predict_test))
print('\n')

# confusion matrix
metrics.plot_confusion_matrix(Xgboost_model,X_test,y_test,values_format='d',cmap='Blues');
plt.grid(False)
plt.title('Confusion Matrix on test set');


metrics.plot_roc_curve(Xgboost_model, X_test, y_test)

# new version ROC for XGB
xgb_predictions_proba = Xgboost_model.predict_proba(X_test)[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, xgb_predictions_proba)
auc = metrics.roc_auc_score(y_test, xgb_predictions_proba) 

plt.plot(fpr3,tpr3,label="XGBoost (AUC="+str(auc3)+")")
plt.title('Receiver operating characteristic(ROC) Curve')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.legend(loc=4)
plt.plot([0, 1], [0, 1],linestyle='--')
plt.show()


y_proba_dt_train = Xgboost_model.predict_proba(X_train)[:, 1]
auc_train = metrics.roc_auc_score(y_train, y_proba_dt_train)

print(f"AUC score for test data: {auc}")
print(f"AUC score for train data: {auc_train}")

import scikitplot as skplt

# pred1 = clf.predict_proba(X_test)
pred4 = Xgboost_model.predict_proba(X_test)

plt.figure(figsize=(7,7))
skplt.metrics.plot_cumulative_gain(y_test, pred4)
plt.show()

# Confusion Matrix Evaluation Metrics

prec = metrics.precision_score(predict_test, y_test)
rec = metrics.recall_score(predict_test, y_test)
f1 = f1_score(predict_test, y_test)

results_xgb = pd.DataFrame([['XGBoost', Xgb_test_score, Xgb_train_score, prec, rec, Xgb_f1_score]],columns=['Model', 'Test Accuracy', 'Train Accuracy', 
                                                                                                           'Precision', 'Recall', 'F1 Score'])
results_xgb

from numpy import loadtxt
from xgboost import XGBClassifier
from xgboost import plot_tree
import matplotlib
import matplotlib.pyplot as plt

	
plot_tree(Xgboost_model, num_trees=3)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(150, 100)
fig.savefig('tree.png')



"""###**Comparison between LR Models**"""

lr_results_total = pd.concat([results, results0, results2, results3], ignore_index=True)
lr_results_total

"""In conclusion, Logistic Regression model with variable selection using RFE has the highest performance in every perspective (Test Accuracy, Precisions, Recall, F1 Score).

###**Comparison between DT Models**
"""

DT_results_total = pd.concat([results_DT1, results_DT2, results_DT5], ignore_index=True)
DT_results_total

"""##**Step 5. Assess**

Model Comparison
"""

# All model Comparison
all_model_results = pd.concat([results_xgb, lr_results_total, DT_results_total], ignore_index=True)
all_model_results.sort_values(by=['Test Accuracy'], ascending=False)

"""**ROC Plot Comparison**"""

# LR ROC Curve Summary
fpr, tpr, thresholds = metrics.roc_curve(y_test, lr_predictions_proba) # Full model
fpr1, tpr1, thresholds = metrics.roc_curve(y_test, lr2_predictions_proba) # variable selected RFE model
fpr2, tpr2, thresholds = metrics.roc_curve(y_test, lr3_predictions_proba) # variable selected model

auc = metrics.roc_auc_score(y_test, lr_predictions_proba)
auc2 = metrics.roc_auc_score(y_test, lr2_predictions_proba)  
auc3 = metrics.roc_auc_score(y_test, lr3_predictions_proba) 

import matplotlib.pyplot as plt
plt.figure(figsize=(10,10))
plt.title('ROC Curve (Logistic Regression Models)')
plt.plot(fpr,tpr, color='olivedrab',label = ['AUC = %0.3f' % auc,'Full Var'])
plt.plot(fpr1,tpr1, color='darkorange',label = ['AUC = %0.3f' % auc2,'RFE Var'])
plt.plot(fpr2,tpr2, color='mediumorchid',label = ['AUC = %0.3f' % auc3,'Univariate+Clustering Var'])

plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],linestyle='--')
plt.axis('tight')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.xlabel('False Positive Rate (1-Specificity)')

# Decision Tree ROC Curve Summary
fpr_1, tpr_1, threshold = metrics.roc_curve(y_test, dt_predictions_proba) # Full model
fpr_2, tpr_2, threshold = metrics.roc_curve(y_test, dt2_predictions_proba) # Prunned model using Gini criterion
fpr_3, tpr_3, threshold = metrics.roc_curve(y_test, dt3_predictions_proba) # Variable selected model

auc_1 = metrics.roc_auc_score(y_test, dt_predictions_proba) 
auc_2 = metrics.roc_auc_score(y_test, dt2_predictions_proba) 
auc_3 = metrics.roc_auc_score(y_test, dt3_predictions_proba) 

import matplotlib.pyplot as plt
plt.figure(figsize=(10,10))
plt.title('ROC Curve (Decision Tree Models)')
plt.plot(fpr_1,tpr_1, color='olivedrab',label = ['AUC = %0.3f' % auc_1,'Full Var'])
plt.plot(fpr_2,tpr_2, color='darkorange',label = ['AUC = %0.3f' % auc_2,'Prunned model (Gini)'])
plt.plot(fpr_3,tpr_3, color='mediumorchid',label = ['AUC = %0.3f' % auc_3,'Univariate+Clustering Var'])

plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],linestyle='--')
plt.axis('tight')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.xlabel('False Positive Rate (1-Specificity)')

"""**All models ROC Curve**"""

#XGBoost ROC
fpr_4, tpr_4, threshold = metrics.roc_curve(y_test, xgb_predictions_proba)
auc_4 = metrics.roc_auc_score(y_test, xgb_predictions_proba) 

plt.figure(figsize=(10,10))
plt.title('ROC Curve (All Models)')
plt.plot(fpr,tpr, color='olivedrab',label = ['AUC = %0.3f' % auc,'LR(Full Var)'])
plt.plot(fpr1,tpr1, color='darkorange',label = ['AUC = %0.3f' % auc2,'LR(RFE)'])
plt.plot(fpr2,tpr2, color='mediumorchid',label = ['AUC = %0.3f' % auc3,'LR(Univariate+Clust)'])
plt.plot(fpr_1,tpr_1, color='aqua',label = ['AUC = %0.3f' % auc_1,'DT(Full Var)'])
plt.plot(fpr_2,tpr_2, color='darkred',label = ['AUC = %0.3f' % auc_2,'DT(Prunned(Gini))'])
plt.plot(fpr_3,tpr_3, color='hotpink',label = ['AUC = %0.3f' % auc_3,'DT(Univariate+Clust)'])
plt.plot(fpr_4,tpr_4, color='darkturquoise',label = ['AUC = %0.3f' % auc_4,'XGB'])


plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],linestyle='--')
plt.axis('tight')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.xlabel('False Positive Rate (1-Specificity)')